setwd("C:/Users/Daniel/OneDrive/My Documents/UBC/ShinyCricket/TestingShiny")
runApp("cricketApp", display.mode = "showcase")
runApp('cricketApp')
runApp('cricketApp')
install.packages("rvest")
library(rvest)
url="http://www.fia.com/events/formula-1-world-championship/season-2015/qualifying-classification"
html(url)
read_html(url)
read_html(url)[1]
str(read_html(url))
url="http://www.espncricinfo.com/england/content/player/303669.html"
read_html(url)
html_node("strong span")
lego_movie <- html("http://www.imdb.com/title/tt1490017/")
lego_movie
html_node("strong span")
lego_movie %>%
html_node("strong span") %>%
html_text() %>%
as.numeric()
lego_movie <- html("http://www.imdb.com/title/tt1490017/")
lego_movie %>%
html_node("image_src")
help(html_nodes)
lego_movie <- html("http://www.espncricinfo.com/england/content/player/303669.html")
lego_movie %>%
html_node("image_src")
Root <- read_html(url)
url="http://www.espncricinfo.com/england/content/player/303669.html"
Root <- read_html(url)
html_nodes(Root, "center")
html_nodes(Root, "href")
read_html("http://www.boxofficemojo.com/movies/?id=ateam.htm")
ateam <- read_html("http://www.boxofficemojo.com/movies/?id=ateam.htm")
html_nodes(ateam, "center")
html_nodes(Root, "link rel="image_src" href="http://www.espncricinfo.com/db/PICTURES/CMS/213800/213843.1.jpg" /")
html_nodes(Root, "centre")
Root
ateam
html_nodes(ateam, "center")
html_nodes(Root, "link")
linkNodes <- html_nodes(Root, "link")
linkNodes
grep('image_src', linkNodes)
imageColumn <- grep('image_src', linkNodes)
linkNodes[imageColumn
]
linkNodes[imageColumn]
html_nodes("#link .itemprop span")
html_nodes("image_src")
html_nodes(Root,"#link .itemprop span")
html_nodes(Root,"image_src")
html_nodes(Root,"link rel=")
html_nodes(Root,"link rel")
html_nodes(Root,"link")
html_text(Root,"image_src")
html_text(Root,"<link rel="image_src" href")
html_text(Root,"link rel="image_src" href")
html_text(Root,"link rel=")
cast <- Root %>%
html_nodes("image_src")
cast
Root %>%
html_nodes("image_src")
Root %>%
html_nodes(Root,"image_src")
html_nodes(Root,"image_src")
Root <- read_html(url)
Root %>%
html_nodes("image_src")
Root
Root %>%
html_nodes("image_src") %>%
html_text()
Root
Root %>%
html_nodes("link") %>%
html_text()
Root %>%
html_nodes("link")
Root %>%
html_nodes("link")[8]
Root %>%
html_nodes("link")
lego_movie <- read_html("http://www.imdb.com/title/tt1490017/")
rating <- lego_movie %>%
html_nodes("strong span")
lego_movie %>%
html_nodes("strong span")
lego_movie %>%
html_nodes("strong span") %>%
html_text()
Root %>%
html_nodes("link") %>%
html_attrs()
linkNodes[imageColumn]
html_attrs(linkNodes[imageColumn])
html_attrs(linkNodes[imageColumn])$href
str(html_attrs(linkNodes[imageColumn]))
str(html_attrs(linkNodes[imageColumn]))[2]
html_attrs(linkNodes[imageColumn])$href[2]
html_attrs(linkNodes[imageColumn])[2]
html_attrs(linkNodes[imageColumn])
str(html_attrs(linkNodes[imageColumn]))
html_attr(linkNodes[imageColumn])
help9html_text()
help(html_text())
help(html_text
)
html_attrs(linkNodes[imageColumn])
help(html_attrs)
html_attr(linkNodes[imageColumn], name= "image_src")
html_attrs(linkNodes[imageColumn])
unlist(html_attrs(linkNodes[imageColumn]))
str(unlist(html_attrs(linkNodes[imageColumn])))
unlist(html_attrs(linkNodes[imageColumn]))[1]
unlist(html_attrs(linkNodes[imageColumn]))[2]
unlist(html_attrs(linkNodes[imageColumn]))[[2]]
library(rvest)
url="http://www.espncricinfo.com/england/content/player/303669.html"
Root <- read_html(url)
html_nodes(Root,"image_src")
linkNodes <- html_nodes(Root, "link")
imageColumn <- grep('image_src', linkNodes)
portraitURL <- unlist(html_attrs(linkNodes[imageColumn]))[[2]]
portraitURL
library(rvest)
url="http://www.espncricinfo.com/india/content/player/35320.html"
playerURL <- read_html(url)
linkNodes <- html_nodes(playerURL, "link")
imageColumn <- grep('image_src', linkNodes)
portraitURL <- unlist(html_attrs(linkNodes[imageColumn]))[[2]]
portraitURL
library(XML)
library(RCurl)
install.packages("XML")
install.packages("XML")
library(XML)
library(RCurl)
getGoogleLinks <- function(google.url) {
doc <- getURL(google.url, httpheader = c("User-Agent" = "R
(2.10.0)"))
html <- htmlTreeParse(doc, useInternalNodes = TRUE, error=function
(...){})
nodes <- getNodeSet(html, "//h3[@class='r']//a")
return(sapply(nodes, function(x) x <- xmlAttrs(x)[["href"]]))
}
getGoogleURL <- function(search.term, domain = '.co.uk', quotes=TRUE)
{
search.term <- gsub(' ', '%20', search.term)
if(quotes) search.term <- paste('%22', search.term, '%22', sep='')
getGoogleURL <- paste('http://www.google', domain, '/search?q=',
search.term, sep='')
}
getGoogleLinks <- function(google.url)
{
doc <- getURL(google.url, httpheader = c("User-Agent" = "R(2.10.0)"))
html <- htmlTreeParse(doc, useInternalNodes = TRUE, error=function(...){})
nodes <- getNodeSet(html, "//a[@href][@class='l']")
return(sapply(nodes, function(x) x <- xmlAttrs(x)[[1]]))
}
search.term <- "cran"
quotes <- "FALSE"
search.url <- getGoogleURL(search.term=search.term, quotes=quotes)
links <- getGoogleLinks(search.url)
links
search.url
getGoogleLinks <- function(google.url) {
doc <- getURL(google.url, httpheader = c("User-Agent" = "R
(2.10.0)"))
html <- htmlTreeParse(doc, useInternalNodes = TRUE, error=function
(...){})
nodes <- getNodeSet(html, "//h3[@class='r']//a")
return(sapply(nodes, function(x) x <- xmlAttrs(x)[["href"]]))
}
links <- getGoogleLinks(search.url)
??
htmlTreeParse
library(XML)
install.packages('XML')
library(XML)
library(RCurl)
links <- getGoogleLinks(search.url)
links
library(URLencode)
library(rvest)
install.packages("URLencode")
getWebsite <- function(name)
{
url = URLencode(paste0("https://www.google.com/search?q=",name))
page <- read_html(url)
results <- page %>%
html_nodes("cite") %>% # Get all notes of type cite. You can change this to grab other node types.
html_text()
result <- results[1]
return(as.character(result)) # Return results if you want to see them all.
}
getWebsite(cricinfo)
URLencode
URLencode(paste0("https://www.google.com/search?q=",danny))
URLencode(paste0("https://www.google.com/search?q=","danny"))
getWebsite("cricinfo")
URLencode(paste0("https://www.google.com/search?q=","ceicinfo"))
URLencode(paste0("https://www.google.com/search?q=","cricinfo"))
url=URLencode(paste0("https://www.google.com/search?q=","cricinfo"))
read_html(url)
page %>%
html_nodes("cite")
page <- read_html(url)
page %>%
html_nodes("cite")
page %>%
html_nodes("cite") %>% # Get all notes of type cite. You can change this to grab other node types.
html_text()
url=URLencode(paste0("https://www.google.com/search?q=","Joe Root cricinfo"))
page <- read_html(url)
page %>%
html_nodes("cite")
page %>%
html_nodes("cite") %>% # Get all notes of type cite. You can change this to grab other node types.
html_text()
player="Joe Root"
read_html(URLencode(paste0("https://www.google.com/search?q=",player)))
paste0("https://www.google.com/search?q=",player)
URLencode(paste0("https://www.google.com/search?q=",player))
read_html(URLencode(paste0("https://www.google.com/search?q=",player)))
searchURL <- URLencode(paste0("https://www.google.com/search?q=",player))
cricinfoPage <- read_html(searchURL)
cricinfoPage
cricinfoPage %>%
html_nodes("cite")
html_nodes(cricinfoPage, "cite")
htmltext(html_nodes(cricinfoPage, "cite"))
html_text(html_nodes(cricinfoPage, "cite"))
findWebsite <- function(player){
searchURL <- URLencode(paste0("https://www.google.com/search?q=",player))
cricinfoPage <- read_html(searchURL)
cricinfoURL <- html_text(html_nodes(cricinfoPage, "cite"))[1]
return(cricinfoURL)
}
findWebsite("Ben Stokes Cricinfo")
help(getForm)
getForm("http://www.google.com/search", hl="en",
lr="", q="r-project", btnG="Search")
site <- getForm("http://www.google.com/search", hl="en",
lr="", q="r-project", btnG="Search")
htmlTreeParse(site)
getForm("http://www.google.com/search", hl="en", lr="",
ie="ISO-8859-1", q="RCurl", btnG="Search")
googleSearchXScraper <- function(input) {
###--- PACKAGES ---###
# load packages
require(RCurl)
require(XML)
###--- LOCAL FUNCTIONS ---###
# I added a wrapper around xpathSApply to deal with cases are NULL and are lost during the list to vector conversion process.
xpathLVApply <- function(doc, xpath.base, xpath.ext, FUN, FUN2 = NULL) {
# get xpaths to each child node of interest
nodes.len <- length(xpathSApply(doc, xpath.base))
paths <- sapply(1:nodes.len, function(i) paste(xpath.base, "[", i, "]", xpath.ext, sep = ""))
# extract child nodes
xx <- lapply(paths, function(xpath) xpathSApply(doc, xpath, FUN))
# perform extra processing if required
if(!is.null(FUN2)) xx <- FUN2(xx)
# convert NULL to NA in list
xx[sapply(xx, length)<1] <- NA
# return node values as a vector
return(as.vector(unlist(xx)))
}
# Determine how to grab html for each element of input
evaluate_input <- function(input) {
# determine which elements of input are files (assumed to contain valid html) and which are not(assumed to be valid URLs)
is.file <- file.exists(input)
# stop if input does not seem to be URLS and/or files
if(sum(is.file) < 1 && length(input) > 1) stop("'input' to googleSearchXScraper() could not be processed.")
# read html from each file
html.files <- lapply(input[is.file], readLines, warn = FALSE)
# read html from each URL
html.webpages <- lapply(input[!is.file], getURL, followlocation = TRUE)
# return all html data as list
return(c(html.files, html.webpages))
}
# construct data frame from the html of a single google search page
get_google_search_df <- function(html) {
# parse html into tree structure
doc <- htmlParse(html)
# construct google search data frame
xpath.base <- "//li[@class='g']"
df <- data.frame(
title = xpathLVApply(doc, xpath.base, "//h3//a[@href]", xmlValue),
url = xpathLVApply(doc, xpath.base, "//h3//a/@href", I),
description = xpathLVApply(doc, xpath.base, "//div[@class='s']", xmlValue),
cached = xpathLVApply(doc, xpath.base, "//div[@class='s']//span[@class='flc']//a/@href",
FUN = function(x) ifelse(length(x[grepl("webcache", x)])<1, "", x[grepl("webcache", x)]),
FUN2 = function(xx) sapply(xx, function(x) ifelse(is.null(x[x!=""]), NA, x[x!=""]))),
similar = xpathLVApply(doc, xpath.base, "//div[@class='s']//span[@class='flc']//a/@href",
FUN = function(x) ifelse(length(x[!grepl("webcache", x)])<1, "", x[!grepl("webcache", x)]),
FUN2 = function(xx) sapply(xx, function(x) ifelse(is.null(x[x!=""]), NA, x[x!=""]))),
stringsAsFactors = FALSE)
# free doc from memory
free(doc)
# return yahoo search dataframe
return(df)
}
###--- MAIN ---##
# STEP 1: Determine input type(s) and grab html accordingly
html.list <- evaluate_input(input)
# STEP 2: get google data frame.
df <- do.call("rbind", lapply(html.list, get_google_search_df))
return(df)
}
input="http://www.google.co.uk/search?aq=f&gcx=w&sourceid=chrome&ie=UTF-8&q=r+project#q=r+project&hl=en&tbo=1&prmdo=1&output=search&source=lnt&tbs=qdr:m&sa=X&ei=qvO_Ttj1KITb8AOPzqT_Aw&ved=0CAoQpwUoBA&fp=1&biw=1920&bih=1086&cad=b&bav=on.2,or.r_gc.r_pw.r_cp.,cf.osb"
googleSearchXScraper(input)
paste("Joe", "Root")
portraitURL <- function(url){
playerURL <- read_html(url)
linkNodes <- html_nodes(playerURL, "link")
imageColumn <- grep('image_src', linkNodes)
portraitURL <- unlist(html_attrs(linkNodes[imageColumn]))[[2]]
return(portraitURl)
}
# finds cricinfo page URL for player input
findWebsite <- function(player){
playerName <- paste(player, "cricinfo")
searchURL <- URLencode(paste0("https://www.google.com/search?q=",playerName))
cricinfoPage <- read_html(searchURL)
cricinfoURL <- html_text(html_nodes(cricinfoPage, "cite"))[1]
return(cricinfoURL)
}
# combines the cricinfo URL and image URL search. Finds image URL
playerImageURLFull <- function(player){
cricinfoURL <- findWebsite(player)
finalURL <- portraitURL(cricinfoURL)
return(finalURL)
}
playerImageURLFull("Joe Root")
findWebsite("Joe Root")
portraitURL("www.espncricinfo.com/england/content/player/303669.html")
url="www.espncricinfo.com/england/content/player/303669.html"
playerURL <- read_html(url)
help(read_html)
library(shiny)
library(yorkr)
library(cricketr)
library(ggplot2)
library(dplyr)
library(magrittr)
library(rvest)
playerURL <- read_html(url)
url="www.espncricinfo.com/england/content/player/303669.html"
playerURL <- read_html(url)
install.packages('xml2')
library('xml2')
install.packages("xml2")
playerURL <- read_html(url)
library(yorkr)
library(cricketr)
library(ggplot2)
library(dplyr)
library(magrittr)
library(rvest)
playerURL <- read_html(url)
read_html("http://en.wikipedia.org/wiki/Brazil_national_football_team")
url="www.espncricinfo.com/england/content/player/303669.html"
playerURL <- read_html(url)
url
html_nodes("www.espncricinfo.com/england/content/player/303669.html", "link")
read_html("http://en.wikipedia.org/wiki/Brazil_national_football_team")
read_html("www.espncricinfo.com/england/content/player/303669.html")
read_html(http://www.espncricinfo.com/england/content/player/303669.html)
read_html("http://www.espncricinfo.com/england/content/player/303669.html")
player="Joe Root"
playerName <- paste(player, "cricinfo")
searchURL <- URLencode(paste0("https://www.google.com/search?q=",playerName))
cricinfoPage <- read_html(searchURL)
cricinfoURL <- html_text(html_nodes(cricinfoPage, "cite"))[1]
cricinfoURL
playerName <- paste(player, "cricinfo")
searchURL <- URLencode(paste0("https://www.google.com/search?q=",playerName))
cricinfoPage <- read_html(searchURL)
cricinfoURL <- paste("http://", html_text(html_nodes(cricinfoPage, "cite"))[1])
cricinfoURL
url(cricinfoURL)
url=cricinfoURL
playerURL <- read_html(url)
library(yorkr)
library(cricketr)
library(ggplot2)
library(dplyr)
library(magrittr)
library(rvest)
# this function finds the URL of player portrait give the cricinfo players' URL
portraitURL <- function(url){
playerURL <- read_html(url)
linkNodes <- html_nodes(playerURL, "link")
imageColumn <- grep('image_src', linkNodes)
portraitURL <- unlist(html_attrs(linkNodes[imageColumn]))[[2]]
return(portraitURl)
}
# finds cricinfo page URL for player input
findWebsite <- function(player){
playerName <- paste(player, "cricinfo")
searchURL <- URLencode(paste0("https://www.google.com/search?q=",playerName))
cricinfoPage <- read_html(searchURL)
cricinfoURL <- paste("http://", html_text(html_nodes(cricinfoPage, "cite"))[1])
return(cricinfoURL)
}
# combines the cricinfo URL and image URL search. Finds image URL
playerImageURLFull <- function(player){
cricinfoURL <- findWebsite(player)
finalURL <- portraitURL(cricinfoURL)
return(finalURL)
}
playerImageURLFull("Joe Root")
player="Joe Root"
player
playerName <- paste(player, "cricinfo")
playerName
searchURL <- URLencode(paste0("https://www.google.com/search?q=",playerName))
cricinfoPage <- read_html(searchURL)
cricinfoURL <- paste("http://", html_text(html_nodes(cricinfoPage, "cite"))[1])
cricinfoURL
urlWeb=cricinfoURL
playerURL <- read_html(urlWeb)
read_html(urlWeb)
cricinfoURL <- paste("http://",html_text(html_nodes(cricinfoPage, "cite"))[1])
urlWeb=cricinfoURL
playerURL <- read_html(urlWeb)
cricinfoURL
help(paste)
cricinfoURL <- paste("http://",html_text(html_nodes(cricinfoPage, "cite"))[1], sep="")
cricinfoURL
playerImageURLFull("Joe Root")
portraitURL <- function(urlWeb){
playerURL <- read_html(urlWeb)
linkNodes <- html_nodes(playerURL, "link")
imageColumn <- grep('image_src', linkNodes)
portraitURL <- unlist(html_attrs(linkNodes[imageColumn]))[[2]]
return(portraitURl)
}
# finds cricinfo page URL for player input
findWebsite <- function(player){
playerName <- paste(player, "cricinfo")
searchURL <- URLencode(paste0("https://www.google.com/search?q=",playerName))
cricinfoPage <- read_html(searchURL)
cricinfoURL <- paste("http://",html_text(html_nodes(cricinfoPage, "cite"))[1], sep="")
return(cricinfoURL)
}
# combines the cricinfo URL and image URL search. Finds image URL
playerImageURLFull <- function(player){
cricinfoURL <- findWebsite(player)
finalURL <- portraitURL(cricinfoURL)
return(finalURL)
}
playerImageURLFull("Joe Root")
player
playerName <- paste(player, "cricinfo")
searchURL <- URLencode(paste0("https://www.google.com/search?q=",playerName))
cricinfoPage <- read_html(searchURL)
cricinfoURL <- paste("http://",html_text(html_nodes(cricinfoPage, "cite"))[1], sep="")
cricinfoURL
urlWeb=cricinfoURL
playerURL <- read_html(urlWeb)
playerURL
linkNodes <- html_nodes(playerURL, "link")
imageColumn <- grep('image_src', linkNodes)
portraitURL <- unlist(html_attrs(linkNodes[imageColumn]))[[2]]
portraitURL
player
cricinfoURL <- findWebsite(player)
cricinfoURL
portraitURL(cricinfoURL)
portraitURL <- function(urlWeb){
playerURL <- read_html(urlWeb)
linkNodes <- html_nodes(playerURL, "link")
imageColumn <- grep('image_src', linkNodes)
portrait <- unlist(html_attrs(linkNodes[imageColumn]))[[2]]
return(portrait)
}
# finds cricinfo page URL for player input
findWebsite <- function(player){
playerName <- paste(player, "cricinfo")
searchURL <- URLencode(paste0("https://www.google.com/search?q=",playerName))
cricinfoPage <- read_html(searchURL)
cricinfoURL <- paste("http://",html_text(html_nodes(cricinfoPage, "cite"))[1], sep="")
return(cricinfoURL)
}
# combines the cricinfo URL and image URL search. Finds image URL
playerImageURLFull <- function(player){
cricinfoURL <- findWebsite(player)
finalURL <- portraitURL(cricinfoURL)
return(finalURL)
}
cricinfoURL <- findWebsite(player)
finalURL <- portraitURL(cricinfoURL)
finalURL
playerImageURLFull("Joe Root")
playerImageURLFull("Tim Bresnan")
playerImageURLFull("Virat Kohli")
shiny::runApp('cricketApp')
runApp('cricketApp')
runApp('cricketApp')
runApp('cricketApp')
runApp('cricketApp')
runApp('cricketApp')
runApp('cricketApp')
runApp('cricketApp')
runApp('cricketApp')
runApp('cricketApp')
runApp('cricketApp')
runApp('cricketApp')
